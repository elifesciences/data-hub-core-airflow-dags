from airflow import DAG
from airflow.operators.python_operator import PythonOperator
import logging
from datetime import timedelta
from pathlib import Path
import os

from data_pipeline.dags.data_pipeline_dag_utils import get_default_args

from data_pipeline.crossref_event_data.extract_crossref_data import write_today, get_crossref_data, get_last_run_day
from data_pipeline.utils.load.bq_data_service import load_file_into_bq
LOGGER = logging.getLogger(__name__)
DEFAULT_ARGS = get_default_args()

DATASET = 'de_dev'
TABLE = 'crossref_event'
STATE_FILENAME ='/home/michael/date_state' #'/usr/local/airflow/dags/date_state'
TEMP_FILE_DIR = '/home/michael/airflow/tempdir'#'/usr/local/airflow/tempfile'
PUBLISHER_ID = '10.7554' # read publisher id from some config file
TEMP_FILE_EXTENSION = '.file'

Path(TEMP_FILE_DIR).mkdir(parents=True, exist_ok=True)

dag = DAG(
    dag_id="Load_Crossref_Event_Into_Bigquery2",
    default_args=DEFAULT_ARGS,
    schedule_interval="@hourly",
    dagrun_timeout=timedelta(minutes=30),
)


def create_python_task(dag, task_id, python_callable):
    return PythonOperator(task_id=task_id, dag=dag, python_callable=python_callable)


def get_last_run_date(**kwargs):
    last_run_date = get_last_run_day(STATE_FILENAME)
    kwargs["ti"].xcom_push(
        key="last_run_date", value=last_run_date
    )


def get_task_run_instance_fullname(task_context):
    return '___'.join ([task_context.get('dag').dag_id, task_context.get('run_id'), task_context.get('task').task_id])


def get_and_transform_crossref_event_data(**kwargs):
    ti = kwargs["ti"]
    last_run_date = ti.xcom_pull(
        key="last_run_date", task_ids="get_last_run_date"
    )
    task_run_instance_fullname= get_task_run_instance_fullname(kwargs)
    downloaded_data_filename = Path.joinpath(Path(TEMP_FILE_DIR), task_run_instance_fullname)

    get_crossref_data(last_run_date, full_temp_file_location=downloaded_data_filename, publisher_id=PUBLISHER_ID)
    kwargs["ti"].xcom_push(key="downloaded_data_filename", value=downloaded_data_filename)

def write_to_bq(**kwargs):
    ti = kwargs["ti"]
    semi_cleansed_data_file_name = ti.xcom_pull(
        key="downloaded_data_filename", task_ids="get_and_transform_crossref_event_data"
    )
    load_file_into_bq(filename=semi_cleansed_data_file_name, dataset_name=DATASET, table_name=TABLE)

def log_last_execution(**kwargs):
    write_today(STATE_FILENAME)

def cleanup_temp_files(**kwargs):
    filename_prefix = kwargs.get('dag').dag_id
    for filename in Path(TEMP_FILE_DIR).glob(''.join([ '**/',  filename_prefix , '*', TEMP_FILE_EXTENSION ]) ):
        os.remove(filename)

get_last_run_date_task = create_python_task(dag, "get_last_run_date", get_last_run_date)
get_and_transform_crossref_event_data_task = create_python_task(
    dag, "get_and_transform_crossref_event_data", get_and_transform_crossref_event_data
)
write_to_bq_task = create_python_task(dag, "write_to_bq", write_to_bq)
log_last_execution_task = create_python_task(dag, "log_last_execution", log_last_execution)
delete_all_previous_temp_file = create_python_task(dag, "cleanup_temp_files", cleanup_temp_files)

[delete_all_previous_temp_file, log_last_execution_task] << write_to_bq_task << get_and_transform_crossref_event_data_task << get_last_run_date_task
