version: '3.4'

x-airflow-env:
  &airflow-env
    - LOAD_EX=n
    - AIRFLOW_HOST=webserver
    - AIRFLOW_PORT=8080
    - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/1
    - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__FERNET_KEY='81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs='
    - AIRFLOW__SMTP__SMTP_HOST=smtp-server
    - AIRFLOW__SMTP__SMTP_STARTTLS=False
    - AIRFLOW__SMTP__SMTP_SSL=False
    - AIRFLOW__SMTP__SMTP_PORT=25
    - AIRFLOW__SMTP__SMTP_MAIL_FROM=do-no-reply@domain.com
    - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.default
    - AIRFLOW__API__ENABLE_EXPERIMENTAL_API=True
    - AIRFLOW__SCHEDULER__FILE_PARSING_SORT_MODE=modified_time
    - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=600
    - AIRFLOW__WEBSERVER__SECRET_KEY='WmZHRmJwd1dCUEp6Xl4zVA=='
    - GOOGLE_APPLICATION_CREDENTIALS=/home/airflow/.config/gcloud/credentials.json
    - GOOGLE_CLOUD_PROJECT=elife-data-pipeline
    - DEPLOYMENT_ENV=${DATA_HUB_DEPLOYMENT_ENV}
    - CROSSREF_CONFIG_FILE_PATH=/home/airflow/sample_data_config/crossref-event/crossref-event-data-pipeline.config.yaml
    - GMAIL_DATA_CONFIG_FILE_PATH=/home/airflow/sample_data_config/gmail-data/gmail-data-pipeline.config.yaml
    - GMAIL_DATA_USER_ID=gmail-end2end-data-test@elifesciences.org
    - GMAIL_ACCOUNT_SECRET_FILE=path-for-gmail-secret
    - GMAIL_E2E_TEST_ACCOUNT_SECRET_FILE=/home/airflow/secrets/gmail_end2end_test_credentials.json
    - GMAIL_THREAD_DETAILS_CHUNK_SIZE=100
    - ELIFE_ARTICLE_XML_CONFIG_FILE_PATH=/home/airflow/sample_data_config/elife-article-xml/elife-article-xml.config.yaml
    - ELIFE_ARTICLE_XML_PIPELINE_SCHEDULE_INTERVAL=${ELIFE_ARTICLE_XML_PIPELINE_SCHEDULE_INTERVAL}
    - EUROPEPMC_PIPELINE_SCHEDULE_INTERVAL=${EUROPEPMC_PIPELINE_SCHEDULE_INTERVAL}
    - EUROPEPMC_CONFIG_FILE_PATH=/home/airflow/sample_data_config/europepmc/europepmc.config.yaml
    - EUROPEPMC_LABSLINK_CONFIG_FILE_PATH=/home/airflow/sample_data_config/europepmc/europepmc-labslink.config.yaml
    - EUROPEPMC_LABSLINK_FTP_PASSWORD_FILE_PATH=/home/airflow/sample_data_config/europepmc/test-ftp-password.txt
    - EUROPEPMC_LABSLINK_FTP_DIRECTORY_NAME_FILE_PATH=/home/airflow/sample_data_config/europepmc/test-ftp-directory-name.txt
    - SEMANTIC_SCHOLAR_PIPELINE_SCHEDULE_INTERVAL=${SEMANTIC_SCHOLAR_PIPELINE_SCHEDULE_INTERVAL}
    - SEMANTIC_SCHOLAR_CONFIG_FILE_PATH=/home/airflow/sample_data_config/semantic-scholar/semantic-scholar.config.yaml
    - SEMANTIC_SCHOLAR_RECOMMENDATION_PIPELINE_SCHEDULE_INTERVAL=${SEMANTIC_SCHOLAR_RECOMMENDATION_PIPELINE_SCHEDULE_INTERVAL}
    - SEMANTIC_SCHOLAR_RECOMMENDATION_CONFIG_FILE_PATH=/home/airflow/sample_data_config/semantic-scholar/semantic-scholar-recommendation.config.yaml
    - SURVEYMONKEY_DATA_CONFIG_FILE_PATH=/home/airflow/sample_data_config/surveymonkey/surveymonkey-data-pipeline.config.yaml
    - SPREADSHEET_CONFIG_FILE_PATH=/home/airflow/sample_data_config/google-spreadsheet/spreadsheet-data-pipeline.config.yaml
    - S3_CSV_CONFIG_FILE_PATH=/home/airflow/sample_data_config/s3-csv/s3-csv-data-pipeline.config.yaml
    - WEB_API_SCHEDULE_INTERVAL=${WEB_API_SCHEDULE_INTERVAL:-}
    - WEB_API_CONFIG_FILE_PATH=${WEB_API_CONFIG_FILE_PATH:-/home/airflow/sample_data_config/web-api/web-api-data-pipeline.config.yaml}
    - GOOGLE_ANALYTICS_CONFIG_FILE_PATH=/home/airflow/sample_data_config/google-analytics/ga-data-pipeline.config.yaml
    - CIVICRM_EMAIL_DATA_CONFIG_FILE_PATH=/home/airflow/sample_data_config/civicrm-email/civicrm-email-report-data-pipeline.config.yaml
    - CIVICRM_API_KEY_FILE_PATH=path-to-civi-api-key
    - CIVICRM_SITE_KEY_FILE_PATH=path-to-civi-site-key
    - PATH_TO_TOGGL_API_TOKEN_FILE=path-to-toggle-api-token
    - MATERIALIZE_BIGQUERY_VIEWS_SCHEDULE_INTERVAL=${MATERIALIZE_BIGQUERY_VIEWS_SCHEDULE_INTERVAL}
    - MATERIALIZE_BIGQUERY_VIEWS_CONFIG_PATH=${MATERIALIZE_BIGQUERY_VIEWS_CONFIG_PATH}
    - MATERIALIZE_BIGQUERY_VIEWS_GCP_PROJECT=${MATERIALIZE_BIGQUERY_VIEWS_GCP_PROJECT}
    - MATERIALIZE_BIGQUERY_VIEWS_DATASET=${MATERIALIZE_BIGQUERY_VIEWS_DATASET}
    - MATERIALIZE_BIGQUERY_VIEWS_VIEW_MAPPING_ENABLED=${MATERIALIZE_BIGQUERY_VIEWS_VIEW_MAPPING_ENABLED}
    - MONITORING_CONFIG_FILE_PATH=/home/airflow/sample_data_config/monitoring/monitoring.config.yaml
    - HEALTH_CHECK_URL=https://hc-ping.com/3549bc53-4bd9-4ef4-9c4a-22aa57c2fb5b
    - SURVEYMONKEY_SECRET_FILE=path-for-surveymonkey
    - TWITTER_ADS_API_CONFIG_FILE_PATH=/home/airflow/sample_data_config/twitter-ads-api/twitter-ads-api.config.yaml
    - BIGQUERY_TO_OPENSEARCH_CONFIG_FILE_PATH=/home/airflow/sample_data_config/opensearch/bigquery-to-opensearch.yaml
    - OPENSEARCH_USERNAME_FILE_PATH=/home/airflow/sample_data_config/opensearch/test-opensearch-username.txt
    - OPENSEARCH_PASSWORD_FILE_PATH=/home/airflow/sample_data_config/opensearch/test-opensearch-password.txt

x-airflow-volumes:
  &airflow-volumes
    - ./sample_data_config:/home/airflow/sample_data_config
    - ./credentials.json:/tmp/credentials.json
    - ~/.aws/credentials:/tmp/.aws-credentials
    - ./.secrets:/home/airflow/secrets
    # only needed for webserver:
    - ./config/webserver_config.py:/opt/airflow/webserver_config.py

services:
    data-hub-dags:
        environment:
            GOOGLE_APPLICATION_CREDENTIALS: /tmp/credentials.json
        volumes:
            - ./credentials.json:/tmp/credentials.json
        build:
            context: .
        image: elifesciences/data-hub-core-dags
        command: ''

    data-hub-dags-dev:
        build:
            context: .
            dockerfile: Dockerfile
            args:
                install_dev: "y"
        environment: *airflow-env
        volumes: *airflow-volumes
        image:  elifesciences/data-hub-core-dags-dev
        command: /bin/sh -c exit 0
        entrypoint: []

    webserver:
        depends_on:
            - worker
        environment: *airflow-env
        image:  elifesciences/data-hub-core-dags-dev
        entrypoint: /entrypoint
        command: webserver
        volumes: *airflow-volumes
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080"]
            interval: 30s
            timeout: 10s
            retries: 10

    smtp-server:
        image:  namshi/smtp

    scheduler:
        depends_on:
            - postgres
        volumes: *airflow-volumes
        image:  elifesciences/data-hub-core-dags-dev
        environment: *airflow-env
        entrypoint: /entrypoint
        command: scheduler

    worker:
        environment: *airflow-env
        depends_on:
          - smtp-server
          - redis
          - scheduler
        volumes: *airflow-volumes
        image: elifesciences/data-hub-core-dags-dev
        entrypoint: /entrypoint
        hostname: worker
        command: >
            bash -c "sudo install -D /tmp/credentials.json -m 644 -t  /home/airflow/.config/gcloud
            && sudo install -D /tmp/.aws-credentials -m 644 --no-target-directory /home/airflow/.aws/credentials
            && airflow celery worker"

    test-ftpserver:
        image: garethflowers/ftp-server
        environment:
            - FTP_PASS=password1
            - FTP_USER=elinks
        volumes: 
            - ./testftp_server_config/vsftpd.conf:/tmp/vsftpd.conf
        command: >
            sh -c "install -D /tmp/vsftpd.conf -m 644 /etc/
            && /usr/sbin/vsftpd"

    wait-for-it:
        image: willwill/wait-for-it

    test-client:
        image:  elifesciences/data-hub-core-dags-dev
        depends_on:
            - scheduler
            - test-ftpserver
            - webserver
        environment: *airflow-env
        volumes: *airflow-volumes
        command: >
            bash -c "sudo install -D /tmp/credentials.json -m 644 -t  /home/airflow/.config/gcloud
            && sudo install -D /tmp/.aws-credentials -m 644 --no-target-directory /home/airflow/.aws/credentials
            && ./run_test.sh with-end-to-end"

    postgres:
        image: postgres:15
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U airflow"]
            interval: 5s
            timeout: 5s
            retries: 5

    redis:
        image: redis:5.0.5
        environment:
            - ALLOW_EMPTY_PASSWORD=yes

    opensearch:
        image: opensearchproject/opensearch:2.9.0
        environment:
            - node.name=opensearch
            - discovery.type=single-node
            - cluster.routing.allocation.disk.threshold_enabled=false
            - cluster.routing.allocation.disk.watermark.low=0mb
            - cluster.routing.allocation.disk.watermark.high=0mb
            - cluster.routing.allocation.disk.watermark.flood_stage=0mb
            - bootstrap.memory_lock=false  # along with the memlock settings below, disables swapping
            - "OPENSEARCH_JAVA_OPTS=-Xms1024m -Xmx1024m"  # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
        ulimits:
            memlock:
                soft: -1
                hard: -1
            nofile:
                soft: 65536  # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
                hard: 65536
        volumes:
            - opensearch-data-airflow-dev:/usr/share/opensearch/data

    opensearch-dashboards:
        image: opensearchproject/opensearch-dashboards:2.9.0
        expose:
            - "5601"
        environment:
            OPENSEARCH_HOSTS: '["https://opensearch:9200"]'

    # flower:
    #     image: elifesciences/data-hub-core-dags-dev
    #     depends_on:
    #         - redis
    #     environment: *airflow-env
    #     ports:
    #         - "5555:5555"
    #     command: celery flower

volumes:
  opensearch-data-airflow-dev:
