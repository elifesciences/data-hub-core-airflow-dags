gcpProjectName: 'elife-data-pipeline'
importedTimestampFieldName: "imported_timestamp"
stateFile:
  defaultBucketName: ci-elife-data-pipeline
  defaultSystemGeneratedObjectPrefix: "airflow-config/{ENV}-s3-csv/state/s3-csv"
s3Csv:
  # Note: the first configuration is used by the end2end test

  #- bucketName: elife-ejp-ftp
  #  objectKeyPattern:
  #    - ejp_query_tool_query_id_15a* # the pattern of the s3 object to be monitored
  # unique data pipeline id to identify the airflow dag run for any data pipeline that runs this config
  #  dataPipelineId: ejp_query_tool_query_id_15a_unique_id
  #  which line the csv header is
  #  headerLineIndex: 3
  #  which line the data values of csv start
  #  dataValuesStartLineIndex: 4
  #  datasetName: "{ENV}"
  #  tableName: 'ejp_query_tool_query_id_15a_test'
  #  tableWriteAppend: True
  #  stateFile:
  #    bucketName: ci-elife-data-pipeline
  #    objectName: "airflow-config/s3-csv/{ENV}-state/query_id_15a.json"
  #  This adds to every record created by this pipeline config a field whose name is given by metadataSchemaFieldName
  #  and whose value in in the line metadataLineIndex of the file
  #  inSheetRecordMetadata:
  #    - metadataSchemaFieldName: 'date_generated'
  #      metadataLineIndex: 1
  #  This adds to every record created by this pipeline config a field whose name is given by metadataSchemaFieldName
  #  and whose value is given in fixedSheetValue
  #  fixedSheetRecordMetadata :
  #    - metadataSchemaFieldName : 'some schema'
  #      fixedSheetValue: "Some value"
  #  This is used to list the names of the different data transformation to run on each record
  #  There is only one optional record processing function implemented at the moment i.e. html_unescape
  #  Information about new optional record processing steps will be provided here as they are implemented
  #  recordProcessingSteps: # html
  #    - html_unescape # as the name suggests, it uescapes html characters from all field values in the record

  - bucketName: ci-elife-data-pipeline
    objectKeyPattern:
      - airflow_test/test-data/s3_csv_data_pipeline_test_data*
    dataPipelineId: s3_csv_test_data_pipeline
    headerLineIndex: 3
    dataValuesStartLineIndex: 4
    datasetName: "{ENV}"
    tableName: 's3_csv_test_data'
    tableWriteAppend: True
    stateFile:
      bucketName: ci-elife-data-pipeline
      objectName: "airflow_test/state/s3-csv/state.json"
    fixedSheetRecordMetadata :
      - metadataSchemaFieldName : 'metadata_schema_name'
        fixedSheetValue: "metadata_value"
    inSheetRecordMetadata:
      - metadataSchemaFieldName: 'date_generated'
        metadataLineIndex: 1
    recordProcessingSteps: # html
      - html_unescape


  # The following configuration items will not be used by end2end test:

  - dataPipelineId: s3_csv_test_sciety_events
    description:
      Load Sciety Events from CSV to BigQuery.
      Parse "payload" as JSON.
    bucketName: sciety-data-extractions
    objectKeyPattern:
      - events-from-cronjob.csv
    headerLineIndex: 0
    dataValuesStartLineIndex: 1
    datasetName: "{ENV}"
    tableName: 's3_csv_test_sciety_event'
    tableWriteAppend: False
    stateFile:
      bucketName: ci-elife-data-pipeline
      objectName: "airflow_test/state/s3-csv/state-sciety-events.json"
    recordProcessingSteps:
      - parse_json_value
    # airflow:
    #   taskParameters:
    #     queue: 'kubernetes'
    #     executor_config:
    #       pod_override:
    #         spec:
    #           containers:
    #             - name: 'base'
    #               resources:
    #                 limits:
    #                   memory: 1gb
    #                   cpu: 1000m
    #                 requests:
    #                   memory: 1gb
    #                   cpu: 100m
